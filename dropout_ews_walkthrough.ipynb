{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An \"Early Warning System\" for Student Drop-out Intervention using a Feature-Weighted Nearest Neighbor Model\n",
    "---------\n",
    "\n",
    "The following notebook walks you through the development of a supervised learning machine learning tool for the early intervention of potentially failing students.  A sample dataset is included as *student_data.csv* in the same directory.  The accompanying Python module \"dropout_ews.py\" contains proprietary functions for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd #Successfully installed pandas-0.19.2\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import dropout_ews as dews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Data Preparation\n",
    "-----------\n",
    "\n",
    "The sample dataset used in this project is included as student_data.csv. The last column 'passed' is the target/label, all other are feature columns.  The CSV contains a header with the following 30 attributes:\n",
    "\n",
    "- __school__ : student's school (binary: \"GP\" or \"MS\")\n",
    "- __sex__ : student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "- __age__ : student's age (numeric: from 15 to 22)\n",
    "- __address__ : student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "- __famsize__ : family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "- __Pstatus__ : parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "- __Medu__ : mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to - __9th grade, 3 - secondary education or 4 - higher education)\n",
    "- __Fedu__ : father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to - __9th grade, 3 - secondary education or 4 - higher education)\n",
    "- __Mjob__ : mother's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "- __Fjob__ : father's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "- __reason__ : reason to choose this school (nominal: close to \"home\", school \"reputation\", \"course\" preference or \"other\")\n",
    "- __guardian__ : student's guardian (nominal: \"mother\", \"father\" or \"other\")\n",
    "- __traveltime__ : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "- __studytime__ : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "- __failures__ : number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "- __schoolsup__ : extra educational support (binary: yes or no)\n",
    "- __famsup__ : family educational support (binary: yes or no)\n",
    "- __paid__ : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "- __activities__ : extra-curricular activities (binary: yes or no)\n",
    "- __nursery__ : attended nursery school (binary: yes or no)\n",
    "- __higher__ : wants to take higher education (binary: yes or no)\n",
    "- __internet__ : Internet access at home (binary: yes or no)\n",
    "- __romantic__ : with a romantic relationship (binary: yes or no)\n",
    "- __famrel__ : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "- __freetime__ : free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "- __goout__ : going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "- __Dalc__ : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "- __Walc__ : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "- __health__ : current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "- __absences__ : number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "Each student has a target that takes two discrete labels:\n",
    "\n",
    "- __passed__ : did the student pass the final exam (binary: yes or no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# READ-IN STUDENT DATA\n",
    "student_data = pd.read_csv(\"student_data.csv\")\n",
    "print \"Student data read successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of student features: 30\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# EXPLORE THE DATA\n",
    "n_students = student_data.shape[0]\n",
    "n_features = student_data.shape[1] - 1\n",
    "n_passed = student_data[\"passed\"].value_counts()[0]\n",
    "n_failed = student_data[\"passed\"].value_counts()[1]\n",
    "grad_rate = float(n_passed)/n_students*100\n",
    "\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of student features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature values:\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT FEATURE AND TARGET DATA\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "\n",
    "print \"Feature values:\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature columns: 30\n",
      "Final feature columns: 48\n",
      "\n",
      "List of features: ['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# CREATE DUMMY BINARY VARS FOR ALL CATEGORICAL FEATURES\n",
    "X_all = dews.preprocess_features(X_all)\n",
    "\n",
    "print \"Original feature columns: {}\".format(n_features)\n",
    "print \"Final feature columns: {}\\n\\nList of features: {}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into Training set and Test set.\n",
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# TEST/TRAIN SPLIT DATA\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = 95\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y_all,train_size=num_train,test_size=num_test,stratify=y_all)\n",
    "\n",
    "print \"Data successfully split into Training set and Test set.\"\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluating Other Supervised Learning Models\n",
    "---------\n",
    "\n",
    "We choose several supervised learning models that are available in scikit-learn, and evaluate their effectiveness on the sample data set for sake of comparison.  They are:\n",
    "\n",
    "- Support Vector Machine (SVM) with RBF kernel\n",
    "- Trimmed Decision Tree\n",
    "- Bayesian Model\n",
    "\n",
    "For each model:\n",
    "- fit the model to the training data\n",
    "- predict labels (for both training and test sets)\n",
    "- measure the F<sub>1</sub> score\n",
    "- repeat this process with different training set sizes (100, 200, 300) while keeping test set constant\n",
    "\n",
    "We product a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "       model     tr_size     tr_time    tr_ptime       tr_f1   tst_ptime      tst_f1\n",
      " SVC_default         100      0.0145      0.0008      0.8784      0.0008      0.7794\n",
      " SVC_default         200      0.0217      0.0008      0.7615      0.0004      0.6038\n",
      " SVC_default         300      0.1148      0.0010      0.7241      0.0006      0.7350\n",
      "   SVC_tuned         100      0.0105      0.0036      0.8375      0.0018      0.8101\n",
      "   SVC_tuned         200      0.0084      0.0044      0.8536      0.0029      0.8025\n",
      "   SVC_tuned         300      0.0165      0.0120      0.8517      0.0026      0.8158\n",
      "\n",
      "\n",
      "       model     tr_size     tr_time    tr_ptime       tr_f1   tst_ptime      tst_f1\n",
      "  DT_default         100      0.0058      0.0073      1.0000      0.0004      0.7402\n",
      "  DT_default         200      0.0053      0.0060      1.0000      0.0007      0.7302\n",
      "  DT_default         300      0.0067      0.0010      1.0000      0.0004      0.7231\n",
      "    DT_tuned         100      0.0016      0.0012      0.8571      0.0005      0.7317\n",
      "    DT_tuned         200      0.0042      0.0009      0.8831      0.0014      0.8169\n",
      "    DT_tuned         300      0.0058      0.0015      0.8889      0.0005      0.7914\n",
      "\n",
      "\n",
      "       model     tr_size     tr_time    tr_ptime       tr_f1   tst_ptime      tst_f1\n",
      "Bayes_default         100      0.0060      0.0017      0.3038      0.0009      0.2564\n",
      "Bayes_default         200      0.0020      0.0023      0.8440      0.0020      0.7752\n",
      "Bayes_default         300      0.0079      0.0052      0.8121      0.0005      0.8345\n"
     ]
    }
   ],
   "source": [
    "# WE VARY THE SIZE OF THE TRAINING SETS FOR THE FOLLOWING MODELS:\n",
    "\n",
    "# Other Model 1: SUPPORT VECTOR MACHINES\n",
    "clf_default = LinearSVC()\n",
    "clf_tuned = SVC(kernel='rbf')\n",
    "\n",
    "dews.train_predict(\"SVC\", X_train, y_train, X_test, y_test, 100, 200, 300, clf_default, clf_tuned)\n",
    "\n",
    "# Other Model 2: DECISION TREE\n",
    "clf_default = DecisionTreeClassifier()\n",
    "clf_tuned = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "dews.train_predict(\"DT\", X_train, y_train, X_test, y_test, 100, 200, 300, clf_default, clf_tuned)\n",
    "\n",
    "# Other Model 3: BAYESIAN CLASSIFICATION\n",
    "clf_default = GaussianNB()\n",
    "\n",
    "dews.train_predict(\"Bayes\", X_train, y_train, X_test, y_test, 100, 200, 300, clf_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Feature-Weighted Nearest Neighbor Model\n",
    "--------\n",
    "\n",
    "We \"feature-weight\" a Nearest Neighbors model by paramater-optimizing a decision tree on the full feature set using GridSearch from the SKLearn library.  This allows us to \"subset\" the full feature set to a set of features that are deemed \"important\" in the determination of student success.\n",
    "\n",
    "For academic background on featuring-weighting in nearest neighbor models, see \"The Utility of Feature Weighting in Nearest-Neighbor Algorithms\", available at http://www.isle.org/~langley/papers/diet.ecml97.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the weighted features with a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Features Identified.\n",
      "Most-important features are ['reason_course', 'studytime', 'failures', 'schoolsup', 'internet', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# make scorer\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"yes\")\n",
    "\n",
    "# Fit a tuned decision tree to training set with ALL features\n",
    "parameters = {'max_depth': range(1,15)}\n",
    "dt = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(dt,parameters,scoring=f1_scorer)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# NOTE: with some test/train splits, a DT model will choose max_depth=1 as the best paramter-\n",
    "# In order retain generality of the subsetting method, we hardcode the max_depth to '3' in dt_tuned classifier.\n",
    "# dt_tuned = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth'])\n",
    "dt_tuned = DecisionTreeClassifier(max_depth=3)\n",
    "dt_tuned.fit(X_train,y_train)\n",
    "\n",
    "# Subset the Dataset by removing features whose 'importance' is zero, \n",
    "# according to a tuned Decision tree in 1.1 \n",
    "X_train_subset = X_train[np.nonzero(dt_tuned.feature_importances_)[0].tolist()]\n",
    "X_test_subset = X_test[np.nonzero(dt_tuned.feature_importances_)[0].tolist()]\n",
    "\n",
    "print \"Weighted Features Identified.\"\n",
    "print \"Most-important features are {}\".format(list(X_train_subset.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final KNN Model with Feature Weighting, and comparison to KNN Model without Feature Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "       model     tr_size     tr_time    tr_ptime       tr_f1   tst_ptime      tst_f1\n",
      "Full_KNN_default         100      0.0010      0.0013      0.8816      0.0010      0.7746\n",
      "Full_KNN_default         200      0.0008      0.0027      0.8505      0.0014      0.7500\n",
      "Full_KNN_default         300      0.0010      0.0053      0.8636      0.0022      0.7518\n",
      "Full_KNN_tuned         100      0.0008      0.0016      0.7947      0.0013      0.7862\n",
      "Full_KNN_tuned         200      0.0009      0.0028      0.8185      0.0015      0.7801\n",
      "Full_KNN_tuned         300      0.0010      0.0056      0.8345      0.0024      0.7518\n",
      "\n",
      "\n",
      "       model     tr_size     tr_time    tr_ptime       tr_f1   tst_ptime      tst_f1\n",
      "Subset_KNN_default         100      0.0008      0.0008      0.8054      0.0006      0.7465\n",
      "Subset_KNN_default         200      0.0007      0.0009      0.8350      0.0006      0.7692\n",
      "Subset_KNN_default         300      0.0008      0.0012      0.8413      0.0006      0.7368\n",
      "Subset_KNN_tuned         100      0.0006      0.0009      0.8024      0.0009      0.8050\n",
      "Subset_KNN_tuned         200      0.0026      0.0046      0.8115      0.0018      0.7974\n",
      "Subset_KNN_tuned         300      0.0035      0.0027      0.8228      0.0009      0.8027\n"
     ]
    }
   ],
   "source": [
    "# out-of-the-box KNN\n",
    "clf_default = KNeighborsClassifier()\n",
    "\n",
    "# Determine the number of nearest neighbors that optimizes accuracy \n",
    "parameters = {'n_neighbors': range(1,30)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_tuned = GridSearchCV(knn,parameters,scoring=f1_scorer)\n",
    "knn_tuned.fit(X_train_subset,y_train)\n",
    "clf_tuned = KNeighborsClassifier(n_neighbors=knn_tuned.best_params_['n_neighbors'])\n",
    "\n",
    "# print performance metrics of default and tuned KNN classifiers on ALL data\n",
    "dews.train_predict(\"Full_KNN\", X_train, y_train, X_test, y_test, 100, 200, 300, clf_default, KNeighborsClassifier(n_neighbors=10))\n",
    "\n",
    "# print performance metrics of default and tuned KNN classifiers on SUBSET of data\n",
    "dews.train_predict(\"Subset_KNN\", X_train_subset, y_train, X_test_subset, y_test, 100, 200, 300, clf_default, clf_tuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
